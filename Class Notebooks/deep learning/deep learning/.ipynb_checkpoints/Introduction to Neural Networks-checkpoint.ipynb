{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Learning</h1>\n",
    "<li>Rough idea: mimic the human brain (not that we know how that works!)\n",
    "<li>Neural networks: learn through examples with no procedural learning algorithm\n",
    "<li>As wikipedia says: <span style=\"color:blue\">vaguely</span> inspired by animal brains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Structure of neural networks</h2>\n",
    "<li>A neural network is a directed acyclic graph\n",
    "<li><span style=\"color:red\">Neurons</span>: nodes\n",
    "<li><span style=\"color:red\">Synapses</span>: connections (edges) between nodes that contain weights\n",
    "<li>a neuron calculates a weighted sum of its input nodes and then decides whether to fire or not (should it react to the input or not)\n",
    "<li><span style=\"color:red\">Activation functions</span>: the function that makes the activation decision\n",
    "<li><span style=\"color:red\">Layers</span>: aggregations of neurons that use the same transformation function (different layers can use different functions)\n",
    "<li><span style=\"color:red\">Input layer</span>: feature values from example cases enter the network here (think of it as the sensory organ of the network)\n",
    "<li><span style=\"color:red\">Output layer</span>: the result (action) layer (classes, continuous values)\n",
    "<li><span style=\"color:red\">Hidden layer</span>: all the layers between the input and output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"neuron.png\",height=500,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example of a network</h2>\n",
    "<small>from: <i>https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rough procedure: \"for dummies\"!</h2>\n",
    "<li>INITIALIZE: randomly assign weights to each connection\n",
    "<li>RUN:\n",
    "<ol>\n",
    "<li>give an example to the network\n",
    "<li>calculate the weighted sum of inputs at each neuron\n",
    "<li>do this layer by layer until you get the output layer values\n",
    "<li>calculate the difference between calculated values and actual values\n",
    "<li>tweak weights in the entire network so that the calculate output gets \"marginally\" closer to the actual value\n",
    "<li>rinse and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A super simple example</h1>\n",
    "<li>Each input case has 3 features, each of which is either a 0 or a 1\n",
    "<li>Classify inputs into either class 0 or class 1\n",
    "<li>We know that if feature 1 is 0, then the class is 0 and if feature 1 is 1, the class is 1\n",
    "<li>We want the net to learn this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"simple_network.png\",height=500,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define inputs and outputs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "#Rick and Ilsa will always have Paris\n",
    "#but, we'll always have numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initialization</h3>\n",
    "<li>generate random weights for every edge in the network\n",
    "<li>we'll use np.random.random (generates random numbers between 0 and 1)\n",
    "<li>and adjust the weights so that they are between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the activation function</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Let's see what our weighted sums look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X,syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Nice. We could say that anything less than 0 is a 0 and anything greater than 0 is a 1\n",
    "<li>Not ideal, because:\n",
    "<ul>\n",
    "<li>learning would be binary and the model would keep switching from class 0 to class 1 \n",
    "<li>what we would like is for learning to be smooth\n",
    "<li>\"hmm, looks like a class 1 but i'll just tweak the probability that it is a class 1 rather than switch entirely to a class 1\"\n",
    "<li>\"that way, over time, I'll get more and more sure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sigmoid function</h2>\n",
    "<small>https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>We don't want to change weights linearly\n",
    "<li>When we're more confident of a weight, we want to change it less\n",
    "<li>When we're less confident of a weight, we want to change it more\n",
    "<li>Sigmoid functions exhibit this property\n",
    "<li>And return values between 0 and 1 \n",
    "<li>Anything .5 or greater is a 1\n",
    "<li>Anything less than .5 is a 0\n",
    "<li>Or, for complex classification problems, we could figure out the right threshold value just like we did with regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "print(sigmoid(.9) - sigmoid(.85))\n",
    "print(sigmoid(.3) - sigmoid(.25))\n",
    "print(sigmoid(.5) - sigmoid(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now let's train the network</h2>\n",
    "<li>First we'll multiply X by the weight array\n",
    "<li>The linear combination we talked about\n",
    "<li>Then apply the sigmoid function to introduce nonlinearity and convert the total to a (0,1) range\n",
    "<li>level_0: First (input layer)\n",
    "<li>level_1: Second (hidden layer - output layer in our case)\n",
    "<li>Then compute the error (y - level_1)\n",
    "<li>Finally adjust the weights\n",
    "<ul>\n",
    "<li>The sigmoid function gives us the rate at which we want to move the weights\n",
    "<li>multiply the error by the slope (derivative) at that point\n",
    "<li>the slope at point x is d/dx sigmoid(x) = sigmoid(x)*(1-sigmoid(x))\n",
    "<li>notice that the slope is lower at the extremes and higher in the middle!\n",
    "<li>Use these deltas to adjust the weights\n",
    "</ul>\n",
    "<li>Repeat with the next set of training cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x,deriv=False):\n",
    "    if deriv:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def run_net(X,y,activation_function=sigmoid,passes=10):\n",
    "    np.random.seed(1)\n",
    "    syn0 = 2*np.random.random((3,1)) - 1    \n",
    "    for i in range(0,passes):\n",
    "        level_0 = X\n",
    "        level_1 = activation_function(np.dot(level_0,syn0))\n",
    "        level_1_error = y - level_1 #error\n",
    "        level_1_delta = level_1_error * activation_function(level_1,True)\n",
    "        syn0 += np.dot(level_0.T,level_1_delta)\n",
    "    return syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = run_net(X,y,sigmoid,10)\n",
    "final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([[1,1,1],[0,1,1],[1,0,0],[0,0,1]])\n",
    "sigmoid(np.dot(test_X,final_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Walkthrough</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0 = X #Input layer\n",
    "level_1 = sigmoid(np.dot(level_0,syn0)) #predicted y's\n",
    "level_1_error = y - level_1 #error\n",
    "level_1_delta = level_1_error * sigmoid(level_1,True) #change factor\n",
    "syn0 += np.dot(level_0.T,level_1_delta)\n",
    "print(level_0)\n",
    "print(level_1,\"\\n\")\n",
    "print(level_1_error,\"\\n\")\n",
    "print(level_1_delta,\"\\n\")\n",
    "print(syn0,\"\\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Great! Let's try a different input</h2>\n",
    "<li>If any two  or three are 1, then the class is 1\n",
    "<li>Otherwise the class is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1],\n",
    "             [1,1,0],\n",
    "             [0,1,0],\n",
    "             [1,0,0],\n",
    "             [1,0,0]])\n",
    "                \n",
    "y = np.array([[0],[1],[1],[1],[1],[0],[0],[0]])\n",
    "\n",
    "final_weights = run_net(X,y,sigmoid,10000)\n",
    "test_X = np.array([[1,1,1],[0,1,1],[1,0,0],[0,0,1]])\n",
    "sigmoid(np.dot(test_X,final_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Terrible!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>The pattern here is non-linear\n",
    "<li>Solution: Add another layer to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Three layer network</h2>\n",
    "<li>Input\n",
    "<li>Hidden\n",
    "<li>Output\n",
    "<li>We need two sets of weights \n",
    "<li>Input layer has 3 nodes\n",
    "<li>Hidden layer has 4 nodes\n",
    "<li>Output layer has 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"multi layer network.png\",height=500,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialize</h2>\n",
    "<li>randomly assign weights at each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculate node outputs at each level</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0 = X\n",
    "level_1 = sigmoid(np.dot(level_0,syn0))\n",
    "level_2 = sigmoid(np.dot(level_1,syn1))\n",
    "syn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Backpropagation</h2>\n",
    "<li>Calculate the error between predicted values (output layer) and actual values\n",
    "<li>propagate these errors back through the net, adjusting weights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_2_error = y - level_2\n",
    "level_2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level_2_delta = level_2_error*sigmoid(level_2,deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Next, propagate the deltas back toward the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level_1_error = level_2_delta.dot(syn1.T)\n",
    "level_1_delta = level_1_error * sigmoid(level_1,deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Calculate the new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn1 += level_1.T.dot(level_2_delta)\n",
    "syn0 += level_0.T.dot(level_1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Putting it all together</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x,deriv=False):\n",
    "    if deriv:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def run_net(X,y,activation_function=sigmoid,passes=10):\n",
    "    import time\n",
    "    np.random.seed(1)\n",
    "    syn0 = 2*np.random.random((3,4)) - 1\n",
    "    syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "    for i in range(passes):\n",
    "        level_0 = X\n",
    "        level_1 = activation_function(np.dot(level_0,syn0))\n",
    "        level_2 = activation_function(np.dot(level_1,syn1))\n",
    "\n",
    "        level_2_error = y - level_2\n",
    "\n",
    "        level_2_delta = level_2_error*activation_function(level_2,deriv=True)\n",
    "\n",
    "        level_1_error = level_2_delta.dot(syn1.T)\n",
    "\n",
    "        level_1_delta = level_1_error * activation_function(level_1,deriv=True)\n",
    "\n",
    "        syn1 += level_1.T.dot(level_2_delta)\n",
    "        syn0 += level_0.T.dot(level_1_delta)\n",
    "    print(level_2)\n",
    "    return syn0,syn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0,syn1 = run_net(X,y,activation_function=sigmoid,passes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Applying the net to test cases</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0 = test_X\n",
    "level_1 = sigmoid(np.dot(level_0,syn0))\n",
    "level_2 = sigmoid(np.dot(level_1,syn1))\n",
    "level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
